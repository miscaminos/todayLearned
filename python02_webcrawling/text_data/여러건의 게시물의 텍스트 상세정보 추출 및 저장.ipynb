{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 건수 만큼 게시물의 상세내역을 추출해서 다양한 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경설정\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"예제: 대한민국 구석구석 사이트의 여행지 정보 수집하기\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query_txt = input(\"크롤링할 키워드는 무엇입니까?\")\n",
    "cnt = int(input(\"크롤링할 검색 건수\"))\n",
    "page_cnt = math.ceil(cnt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dir = input(\"결과 파일들을 저장할 폴더명만 쓰세요:(C:\\\\Study\\\\Python\\\\notebook\\\\data\\\\)\")\n",
    "n = time.localtime()\n",
    "s = '{}-{}-{}-{}-{}-{}' .format(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "\n",
    "ff_name = f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "fc_name = f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.csv'\n",
    "fx_name = f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "path=\"C:\\\\Study\\\\Python\\\\datadown\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"https://korean.visitkorea.or.kr/\")\n",
    "time.sleep(random.randrange(2,5))\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_xpath('//*[@id=\"chkForm01\"]').click()\n",
    "except:\n",
    "    print('코로나 창이 없습니다.')\n",
    "    \n",
    "element = driver.find_element_by_id(\"inp_search\")\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"크롤링할 키워드{}\" .format(query_txt))\n",
    "print(\"크롤링할 총 페이지 갯수:\", page_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2=[]\n",
    "contents2=[]\n",
    "\n",
    "no=1\n",
    "for x in range(1,page_cnt+1):\n",
    "    print(\"{} 페이지 내용의 수집을 시작합니다\".format(x))\n",
    "    \n",
    "    #한페이지에 10개의 게시물이 있기때문에 range(1,11)\n",
    "    for i in range(1,11):\n",
    "        #요청한 검색건수에 도달하면 for loop나오기\n",
    "        if no > cnt:\n",
    "            break\n",
    "            \n",
    "        #각 게시글 제목 누르기    \n",
    "        try:\n",
    "            driver.find_element_by_xpath('''//*[@id=\"listBody\"]/ul/li[%s]/div[2]/div[1]/a'''%i).click()\n",
    "        except:\n",
    "            #ul의 li들중에 광고banner같은 것들은 skip하기위해\n",
    "            continue\n",
    "\n",
    "        time.sleep(random.randrange(2,5))\n",
    "        \n",
    "        html = driver.page_source # 상세페이지 HTML가져오기\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #상세페이지에서 내용부분(div) 가져오기\n",
    "        content_list = soup.find('div','wrap_contView')\n",
    "        \n",
    "        try:\n",
    "        #내용태그에서 텍스트만 가져온다\n",
    "            content_1 = content_list.find('div','txt_p').get_text()\n",
    "        except:\n",
    "            #div class이름이 txt_p가 아닌경우\n",
    "            try:\n",
    "                content_1 = content_list.find('div','inr').get_text()\n",
    "                #content_1 = content_0.find('p').get_text()\n",
    "            except:\n",
    "                driver.back()\n",
    "                #continue를 넣지않으면 driveback()후 다음 상세페이지로 이동하지않고 계속 현페이지에 머문다\n",
    "                continue\n",
    "                \n",
    "        print(no, ': ', content_1) #번호와, 추출한 데이터 출력한다.\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        f = open(ff_name,'a',encoding='utf-8')\n",
    "        f.write(str(no)+':'+str(content_1)+\"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        no2.append(no)\n",
    "        contents2.append(content_1)\n",
    "        \n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        no += 1\n",
    "    \n",
    "    if x > page_cnt+1:\n",
    "        break\n",
    "    x += 1\n",
    "    \n",
    "    if(x%5==1):\n",
    "        driver.find_element_by_link_text(\"다음\").click()\n",
    "    else:\n",
    "        driver.find_element_by_link_text(\"{}\".format(x)).click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju = pd.DataFrame()\n",
    "jeju['번호']=no2\n",
    "jeju['내용']=contents2\n",
    "\n",
    "jeju.to_csv(fc_name, encoding=\"utf-8-sig\",index=False)\n",
    "jeju.to_excel(fx_name, index=False)\n",
    "\n",
    "e_time = time.time()\n",
    "d_time = e_time - s_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#요약정보 보여주기\n",
    "print(\"\\n\") \n",
    "print(\"=\" *80)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(d_time,1))\n",
    "print(\"파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *80)\n",
    " \n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#practice \n",
    "#한번에 실행 coding from scratch:\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "###빠트린곳: report형식으로 어떤 주제의 데이터 수집인지 명시필요\n",
    "print(\"=\"*80)\n",
    "print(\"예제:대한민국 구석구석 사이트의 여행지 정보 수집하기\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#데이터 검색 조건\n",
    "query_txt = input('검색할 키워드 입력:')\n",
    "cnt = int(input('요청 검색건수 입력:'))\n",
    "cnt_per_page = 10\n",
    "page_cnt = math.ceil(cnt/cnt_per_page)\n",
    "\n",
    "#검색 결과를 저장할 디렉토리 생성\n",
    "dir_name=input('저장할 폴더 경로 입력:(C:\\\\Study\\\\Python\\\\notebook\\\\data\\\\)')\n",
    "n=time.localtime()\n",
    "s=\"{}-{}-{}-{}-{}\".format(n.tm_year,n.tm_mon,n.tm_mday,n.tm_hour,n.tm_min)\n",
    "print('\\n')\n",
    "\n",
    "###빠트린곳: 디렉토리생성 코드!!!! (os를 import한 이유...)\n",
    "os.makedirs(dir_name+s+'-'+query_txt)\n",
    "\n",
    "#검색 결과를 저장 할 파일 이름 생성\n",
    "ft_name=dir_name+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "fc_name=dir_name+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.csv'\n",
    "fx_name=dir_name+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.xlsx'\n",
    "\n",
    "###빠트린곳: 검색+수집에 걸린 시간 측정위해 시작시간 확인\n",
    "s_time = time.time()\n",
    "\n",
    "#드라이버 생성 및 웹사이트 열기\n",
    "path = \"C:\\\\Study\\\\Python\\\\datadown\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"https://korean.visitkorea.or.kr/\")\n",
    "\n",
    "###빠트린곳: 중간중간 time.sleep넣기\n",
    "time.sleep(random.randrange(2,5))\n",
    "\n",
    "#해당 웹사이트의 홈페이지 alert msg창이 있다면, 닫기\n",
    "try:\n",
    "    driver.find_element_by_xpath('//*[@id=\"chkForm01\"]').click()\n",
    "except:\n",
    "    print('코로나 창이 없습니다')\n",
    "\n",
    "###틀린곳: element라는 변수에 담아야함!!\n",
    "#검색 키워드 입력 후 결과 페이지 조회\n",
    "element = driver.find_element_by_id(\"inp_search\")\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "html=driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "###틀린곳: x(page_cnt의 카운터)별로 forloop돌기전에!! loop밖에서 빈리스트 생성해야지\n",
    "no2 = []\n",
    "contents2 =[]\n",
    "#no 초기화. 수집 할 게시물 갯수 카운터\n",
    "no=1\n",
    "\n",
    "#검색결과 리스트페이지에서 하나씩 상세페이지를 조회하여 내용 받아오기\n",
    "for x in range(1,page_cnt+1):\n",
    "    #검색목록 페이지의 ul내 li중에 게시글 외에 다른게 있다면, i의 count를 올라가지만, 내용을 출력되지않는다.\n",
    "    print(\"{}번 검색결과 페이지의 상세 내용 입니다.==============\".format(x))\n",
    "    \n",
    "    for i in range(1,cnt_per_page+2):\n",
    "        \n",
    "        ###틀린곳: for-loop을 나올 코드\n",
    "        #range의 숫자만큼 반복하는데 왜 break할 조건을 또 명시해야하지?\n",
    "        if no > cnt:\n",
    "            break\n",
    "            \n",
    "        ###틀린곳: driver가 element를 찾는다!!\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@id=\"listBody\"]/ul/li[%s]/div[2]/div[1]/a' %i).click()\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        time.sleep(random.randrange(2,5))\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        \n",
    "        content_list = soup.find('div','wrap_contView')\n",
    "        \n",
    "        ###틀린곳: .get_text()를 써야지만 내부 문자내용만 받아올수있다\n",
    "        try:\n",
    "            content_1 = content_list.find('div','txt_p').get_text()\n",
    "        except:\n",
    "            try:\n",
    "                content_1 = content_list.find('div','inr').get_text()\n",
    "            except:\n",
    "                #아래 txt파일에 쓰는 코드를 거치지않고 바로 driver.back() 검색목록으로 뒤로가기\n",
    "                driver.back()\n",
    "                continue\n",
    "        \n",
    "        print(no,\":\", content_1)\n",
    "        print('\\n')\n",
    "        \n",
    "        ###틀린곳: 여기서 바로 content_1을 txt파일에 쓴다.\n",
    "        f = open(ft_name,'a',encoding='utf-8')\n",
    "        f.write(str(no)+':'+str(content_1)+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "        no2.append(no)   \n",
    "        no += 1\n",
    "        contents2.append(content_1)\n",
    "        \n",
    "        ###틀린곳: txt파일에 상세페이지내용1개 쓰고, 리스트contents2에도 더한 후,\n",
    "        #다시 검색목록 페이지로 뒤로가기\n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "        print('i:',i)\n",
    "    \n",
    "    x+=1\n",
    "    \n",
    "    if(x%5 ==1):\n",
    "        driver.find_element_by_link_text(\"다음\").click()\n",
    "    else:\n",
    "        driver.find_element_by_link_text(\"{}\".format(x)).click()   \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "tripinfo = pd.DataFrame()\n",
    "tripinfo['번호']=no2\n",
    "tripinfo['내용']=contents2\n",
    "\n",
    "tripinfo.to_csv(fc_name, encoding='utf-8-sig', index=False)\n",
    "tripinfo.to_excel(fx_name, index=False)\n",
    "\n",
    "e_time = time.time()\n",
    "d_time = e_time-s_time\n",
    "\n",
    "print(\"검색결과 추출 시간:{}\".format(d_time))\n",
    "print(\"저장된 검색결과 파일:\\n {} \\n, {} \\n, {}\".format(ft_name, fc_name, fx_name))\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice finding element tag:\n",
    "\n",
    "content_list = soup.find('ul','list_thumType type1')\n",
    "print(type(content_list))\n",
    "\n",
    "photo = content_list.find('div','photo')\n",
    "print(type(photo))\n",
    "print(photo.select('a[href]'))\n",
    "\n",
    "txt = content_list.find('div','area_txt')\n",
    "print(type(txt))\n",
    "\n",
    "popMenu = content_list.find('div','pop_subMenu')\n",
    "print(type(popMenu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
